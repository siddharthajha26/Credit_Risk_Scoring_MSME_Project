{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277d809c-f725-410d-aae1-5e045d5d6aae",
   "metadata": {},
   "source": [
    "# Rating Scale\n",
    "\n",
    "A Rating Scale is a standardized assesment that that reflects the creditworthiness of a borrower,Financial Instrument or and Entity which indicates its associated risk level.It is a measure of likelyhood of default or financial distress often represented in the form of letters(AAA,BB+) or Number (300-900).\n",
    "\n",
    "To develop the rating scale of a portfolio as in the current case we need the features driving the PD (Borrower/Entity Specific Attributes),the target variable and The Predicted Probabities for the trained data as per the ML model used.In the current case ,we shall use the Logistic Regression Model.\n",
    "So,First we shall Import our data from PART2 where we have stored the dataset and the PD values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8e4c66d-99d8-4cdc-8d21-d8a3588b8c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r train_data_logistic\n",
    "%store -r test_data_logistic\n",
    "%store -r validation_data_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a78af9-2d12-4b6a-91fc-fe8ef3688a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "685b64ff-6558-4ac0-af2a-0d7c5cdafac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(594, 67)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_logistic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b14b1-624d-488b-9197-b7c409aa7a50",
   "metadata": {},
   "source": [
    "## Master Scale\n",
    "\n",
    "In a credit risk model, a master scale (or rating scale) serves as a standardized grid of rating grades (e.g., AAA to D or 1‚Äì10) that maps internal scores or probabilities of default (PD) to specific risk categories often categorised into bins. It provides a common language for risk across different portfolios\n",
    "\n",
    "    We shall create a master scale that best describes the target variable according to the given parameters.\n",
    "\n",
    "    Parameters:\n",
    "    data : pandas DataFrame\n",
    "        The dataset.\n",
    "    default_flag : str\n",
    "        The column name of the default flag(0,1).In our case it is the PD column in the train_data_logistic that contains the predicted default flag.\n",
    "    PD : str\n",
    "        The column name of the PD variable In our case it is the PD_p column in the train_data_logistic which indicated the probability of the default.\n",
    "    bin_number : int\n",
    "        The number of bins to create for binning the PD variable.Here we shall split the data into 10 bins.The choice of number of bins is coded below .\n",
    "        \n",
    " -**Several rules of thumb from statistics offer a starting point for a reasonable number of bins (or bin width):**\n",
    "        \n",
    "    -Square-root rule: A common and simple starting point, particularly for general visualization, is to use the square root of the number of observations (N): bins = sqrt(N).\n",
    "     -Sturges' formula: This formula is well-suited for normally distributed data: bins = log2(N) + 1\n",
    "     .\n",
    "     -Scott's normal reference rule: This method calculates an optimal bin width based on the data's standard deviation and sample size, then derives the number of bins from that width.\n",
    "     -Freedman-Diaconis rule: Often considered more robust than Sturges' formula as it uses the interquartile range (IQR), making it less sensitive to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c457a-9045-4c72-a026-ab5f2bc7db07",
   "metadata": {},
   "source": [
    " **Considerations:**\n",
    "   - Assumes input data (`data`) includes numeric columns suitable for binning and analysis.\n",
    "   - Provides insights into the distribution and risk assessment based on the Probability of Default (`PD`) variable  across bin categories\n",
    "   - Enables customization through parameters like binning granularity (`bin_number`) for tailored risk analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5efaabbe-0c74-4511-9991-b665e3b3fa25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how can we decide the value of n i.e no of bins over which the sample should be divided such that there is a proportinate distribution of \n",
    "# data in each of the bins as in the original data\n",
    "# We will try Freedman Diaconis rule\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data=np.array(train_data_logistic['PD'])\n",
    "Q1=np.percentile(data,0.25)\n",
    "Q3=np.percentile(data,0.75)\n",
    "IQR=Q3-Q1\n",
    "IQR\n",
    "# The IQR is 0 ,so it does not convey any useful information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11bfe5c2-ef89-4589-9559-2955aa40be32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.214319120800766"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sturges Method\n",
    "import math\n",
    "length=math.log2(len(train_data_logistic['PD']))\n",
    "length\n",
    "n=length +1\n",
    "n\n",
    "#  This method gives us the number of bins as 10.21.Hence we shall use 10 as the number of bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "784e5bd5-963e-431b-89f3-b35fa9f38ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455    (-0.001, 0.1]\n",
       "516    (-0.001, 0.1]\n",
       "147       (0.6, 0.7]\n",
       "789       (0.9, 1.0]\n",
       "121    (-0.001, 0.1]\n",
       "           ...      \n",
       "637       (0.2, 0.3]\n",
       "425       (0.1, 0.2]\n",
       "315       (0.9, 1.0]\n",
       "225    (-0.001, 0.1]\n",
       "342    (-0.001, 0.1]\n",
       "Name: PD_Bin, Length: 594, dtype: category\n",
       "Categories (10, interval[float64, right]): [(-0.001, 0.1] < (0.1, 0.2] < (0.2, 0.3] < (0.3, 0.4] ... (0.6, 0.7] < (0.7, 0.8] < (0.8, 0.9] < (0.9, 1.0]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Custom Binning:\n",
    "# We shall implement custom binning logic on the PD variable (`PD`) of the input data (`data`) using `pd.cut`.\n",
    "data=train_data_logistic\n",
    "default_flag=\"PD\"\n",
    "PD=\"PD_p\"\n",
    "bin_number=10\n",
    "# Implement custom binning logic here\n",
    "data['PD_Bin'] = pd.cut(data[PD], bins=bin_number)# pd.cut is used to convert continuous PD_p values into 10 individual bins.This will place each of the data points in one of the 10 bins\n",
    "data['PD_Bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "041c1b76-95ed-4b59-aab1-0d4b5e85faf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_17048\\3574640931.py:2: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  summary = data.groupby('PD_Bin').agg({\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>PD_Bin</th>\n",
       "      <th colspan=\"2\" halign=\"left\">PD</th>\n",
       "      <th colspan=\"2\" halign=\"left\">PD_p</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-0.001, 0.1]</td>\n",
       "      <td>370</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006688</td>\n",
       "      <td>0.014793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.1, 0.2]</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137085</td>\n",
       "      <td>0.026175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.2, 0.3]</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225923</td>\n",
       "      <td>0.023082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.3, 0.4]</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.362715</td>\n",
       "      <td>0.036455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.4, 0.5]</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469115</td>\n",
       "      <td>0.022546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(0.5, 0.6]</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.535657</td>\n",
       "      <td>0.034773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0.6, 0.7]</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.654010</td>\n",
       "      <td>0.037396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(0.7, 0.8]</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.753022</td>\n",
       "      <td>0.026488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(0.8, 0.9]</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0.858946</td>\n",
       "      <td>0.023543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(0.9, 1.0]</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>0.981224</td>\n",
       "      <td>0.023719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          PD_Bin    PD           PD_p          \n",
       "                 count  sum      mean       std\n",
       "0  (-0.001, 0.1]   370    0  0.006688  0.014793\n",
       "1     (0.1, 0.2]    17    0  0.137085  0.026175\n",
       "2     (0.2, 0.3]    12    0  0.225923  0.023082\n",
       "3     (0.3, 0.4]     7    0  0.362715  0.036455\n",
       "4     (0.4, 0.5]     6    0  0.469115  0.022546\n",
       "5     (0.5, 0.6]     7    7  0.535657  0.034773\n",
       "6     (0.6, 0.7]    10   10  0.654010  0.037396\n",
       "7     (0.7, 0.8]    12   12  0.753022  0.026488\n",
       "8     (0.8, 0.9]    19   19  0.858946  0.023543\n",
       "9     (0.9, 1.0]   134  134  0.981224  0.023719"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate data based on PD bins(count,sum,mean,std)\n",
    "summary = data.groupby('PD_Bin').agg({\n",
    "    default_flag: ['count', 'sum'],\n",
    "    PD: ['mean', 'std']\n",
    "}).reset_index()\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b96c585-ba68-492e-992c-f7ab2078f555",
   "metadata": {},
   "source": [
    "Calculating Weight of Evidence (WoE) involves transforming a feature's categories (bins) into numerical values, measuring their predictive power for a binary outcome (good/bad) using the natural log of the ratio of \"good\" to \"bad\" proportions:\n",
    "WoE = $ ln(\\frac {\\text Pecentage Good in Bin}{\\text Percentage Bad in Bin})$ , with steps including binning continuous variables and ensuring non-zero values. It quantifies how much a category in a bin shifts the odds towards \"good\" versus \"bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68558b33-3481-40da-88b0-7d9184157ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.006688\n",
       "1    0.137085\n",
       "2    0.225923\n",
       "3    0.362715\n",
       "4    0.469115\n",
       "5    0.535657\n",
       "6    0.654010\n",
       "7    0.753022\n",
       "8    0.858946\n",
       "9    0.981224\n",
       "Name: (PD_p, mean), dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[(PD, 'mean')] # This gives the mean value of PD_p column across each of the bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9dcc495-7625-4773-9d47-a8111c614b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r train_data_corr_eliminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91f91ce2-e71c-4e51-8e67-4c4419e3abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate WOE and Score using custom logic\n",
    "# WOE and Score Calculation\n",
    "# Computes Weight of Evidence (WOE) and Score metrics using custom logic.\n",
    "# Calculates counts and distributions of 'Good' and 'Bad' observations.\n",
    "# Computes Bad Rate and Probability of Default (`PD`).\n",
    "# Derives Score using the formula `-ln(PD / (1 - PD)) * 100`.\n",
    "def master_scale(data, default_flag, PD, bin_number):\n",
    "    # Implement custom binning logic here\n",
    "    data['PD_Bin'] = pd.cut(data[PD], bins=bin_number)\n",
    "    # Aggregate data based on PD bins\n",
    "    summary = data.groupby('PD_Bin').agg({\n",
    "        default_flag: ['count', 'sum'],\n",
    "        PD: ['mean', 'std']\n",
    "    }).reset_index()\n",
    "    summary['Total.Observations'] = summary[(default_flag, 'count')]# Total Observations is the count of the total observations in the PD column in each bin\n",
    "    summary['Bad.Count'] = summary[(default_flag, 'sum')]# Since default is indicated by 1's in PD column,therefore the sum  of 1's gives the total defaults in the column in each bin\n",
    "    summary['Good.Count'] = summary[(default_flag, 'count')] - summary[(default_flag, 'sum')]# Goods or 0's can be calculated by subtracting the Bad's from Total Observations in reach bin.\n",
    "    \n",
    "    \n",
    "    summary['Good.Distr'] = summary['Good.Count'] / len(data)# If we divide the total good observation by the length of the data which is the total number \n",
    "    #of rows in the bins,We shall get the no. of good observations per row in each bin which may further represent the distribution of goods across the sample in each of the bins\n",
    "    summary['Bad.Distr'] = summary['Bad.Count'] / len(data)# Similarly distribution of Bad's can be determined\n",
    "    \n",
    "    summary['Total.Distr'] = summary['Total.Observations'] / len(data)# Total distribution represent the information about Goods's or Bad's per row in each bin.\n",
    "\n",
    "    summary['Bad.Rate'] = summary['Bad.Count'] / summary['Total.Observations'] # Instances of Bad's is the frequency of occurence of Bad's from the total observations per row in each bin.\n",
    "    \n",
    "    summary['PD'] = summary[(PD, 'mean')]# Mean of the Probability of default PD_p column in each bin.\n",
    "    # WOE score is evaluated using the formula:\n",
    "    summary['Score'] = -np.log(summary[(PD, 'mean')] / (1 - summary[(PD, 'mean')])) * 100\n",
    "    # Select and rename columns\n",
    "    woe_summary = summary[['PD_Bin', 'Total.Observations', 'Total.Distr', 'Good.Count', 'Bad.Count',\n",
    "                           'Good.Distr', 'Bad.Distr', 'Bad.Rate', 'PD', 'Score']]\n",
    "    woe_summary.columns = ['Final.PD.Range', 'Total.Observations', 'Total.Distr', 'Good.Count', 'Bad.Count',\n",
    "                           'Good.Distr', 'Bad.Distr', 'Bad.Rate', 'PD', 'Score']\n",
    "\n",
    "    return woe_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b30f5644-ba07-4bfd-aaea-cd157111de22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_17048\\2787648883.py:11: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  summary = data.groupby('PD_Bin').agg({\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final.PD.Range</th>\n",
       "      <th>Total.Observations</th>\n",
       "      <th>Total.Distr</th>\n",
       "      <th>Good.Count</th>\n",
       "      <th>Bad.Count</th>\n",
       "      <th>Good.Distr</th>\n",
       "      <th>Bad.Distr</th>\n",
       "      <th>Bad.Rate</th>\n",
       "      <th>PD</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-0.001, 0.1]</td>\n",
       "      <td>370</td>\n",
       "      <td>0.622896</td>\n",
       "      <td>370</td>\n",
       "      <td>0</td>\n",
       "      <td>0.622896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006688</td>\n",
       "      <td>500.071105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.1, 0.2]</td>\n",
       "      <td>17</td>\n",
       "      <td>0.028620</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0.023569</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.137085</td>\n",
       "      <td>183.971446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.2, 0.3]</td>\n",
       "      <td>12</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013468</td>\n",
       "      <td>0.006734</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.225923</td>\n",
       "      <td>123.147919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.3, 0.4]</td>\n",
       "      <td>7</td>\n",
       "      <td>0.011785</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006734</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.362715</td>\n",
       "      <td>56.359834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.4, 0.5]</td>\n",
       "      <td>6</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.469115</td>\n",
       "      <td>12.369724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(0.5, 0.6]</td>\n",
       "      <td>7</td>\n",
       "      <td>0.011785</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.535657</td>\n",
       "      <td>-14.287230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0.6, 0.7]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.016835</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008418</td>\n",
       "      <td>0.008418</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.654010</td>\n",
       "      <td>-63.671163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(0.7, 0.8]</td>\n",
       "      <td>12</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.008418</td>\n",
       "      <td>0.011785</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.753022</td>\n",
       "      <td>-111.479456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(0.8, 0.9]</td>\n",
       "      <td>19</td>\n",
       "      <td>0.031987</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.858946</td>\n",
       "      <td>-180.656335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(0.9, 1.0]</td>\n",
       "      <td>134</td>\n",
       "      <td>0.225589</td>\n",
       "      <td>3</td>\n",
       "      <td>131</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.220539</td>\n",
       "      <td>0.977612</td>\n",
       "      <td>0.981224</td>\n",
       "      <td>-395.621415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Final.PD.Range  Total.Observations  Total.Distr  Good.Count  Bad.Count  \\\n",
       "0  (-0.001, 0.1]                 370     0.622896         370          0   \n",
       "1     (0.1, 0.2]                  17     0.028620          14          3   \n",
       "2     (0.2, 0.3]                  12     0.020202           8          4   \n",
       "3     (0.3, 0.4]                   7     0.011785           4          3   \n",
       "4     (0.4, 0.5]                   6     0.010101           3          3   \n",
       "5     (0.5, 0.6]                   7     0.011785           1          6   \n",
       "6     (0.6, 0.7]                  10     0.016835           5          5   \n",
       "7     (0.7, 0.8]                  12     0.020202           5          7   \n",
       "8     (0.8, 0.9]                  19     0.031987           1         18   \n",
       "9     (0.9, 1.0]                 134     0.225589           3        131   \n",
       "\n",
       "   Good.Distr  Bad.Distr  Bad.Rate        PD       Score  \n",
       "0    0.622896   0.000000  0.000000  0.006688  500.071105  \n",
       "1    0.023569   0.005051  0.176471  0.137085  183.971446  \n",
       "2    0.013468   0.006734  0.333333  0.225923  123.147919  \n",
       "3    0.006734   0.005051  0.428571  0.362715   56.359834  \n",
       "4    0.005051   0.005051  0.500000  0.469115   12.369724  \n",
       "5    0.001684   0.010101  0.857143  0.535657  -14.287230  \n",
       "6    0.008418   0.008418  0.500000  0.654010  -63.671163  \n",
       "7    0.008418   0.011785  0.583333  0.753022 -111.479456  \n",
       "8    0.001684   0.030303  0.947368  0.858946 -180.656335  \n",
       "9    0.005051   0.220539  0.977612  0.981224 -395.621415  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_scale_data = master_scale(train_data_logistic, \"default_flag\", \"PD_p\", 10)\n",
    "master_scale_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91cf4f02-54a1-4620-b750-49ab3d235ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'master_scale_data' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store master_scale_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56afa0d3-633e-4596-8332-000ec4dc9d18",
   "metadata": {},
   "source": [
    "The WoE score is nothing but the logarithm of the  ratio of mean of probabilities of default vs mean of Prob. of non- default in each of the bins and hence represents mean probabilty of odds across each bin.\n",
    "So , the scores can be calculated from the predicted probailities of default using the equation Score: log(PD/1-PD).\n",
    "We insert a temporary column in the data and assign it a value 1 and then calculate the Scores from the predicted probabilites as shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9df2e21-a3f4-4576-ad05-db0a2ededee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(data):\n",
    "    data[\"temp\"]=1\n",
    "    data[\"Scores\"]=np.log(data[\"PD_p\"]/(data[\"temp\"]-data[\"PD_p\"]))\n",
    "    data.drop(\"temp\",axis =1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1abe9bd6-16dc-4324-a085-5db0f55d59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=calculate_scores(train_data_logistic)\n",
    "test_data=calculate_scores(test_data_logistic)\n",
    "validation_data=calculate_scores(validation_data_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dddff0b6-9131-44de-aca9-5b60f0101f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_data' (DataFrame)\n",
      "Stored 'test_data' (DataFrame)\n",
      "Stored 'validation_data' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store train_data\n",
    "%store test_data\n",
    "%store validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2968517-88d5-42cf-a613-1deb96c4fb38",
   "metadata": {},
   "source": [
    "# Credit Score Scaling\n",
    " \n",
    "Credit Scorecard will show how points represented by the bins generated from the probability of defaults as predicted by the Logistic Regression model. Generating the score points will involve scaling calculations from the logistic regression parameters and WoE(s) from grouped values inside each column in the woe_summary.\n",
    "Formula for calculating attribute score:\n",
    "\n",
    "Attribute Score=$ - \\text ((WoE)_j * \\beta_i + \\frac {\\text a}{\\text n}) * (\\text Factor) + \\frac{\\text Offset}{n})$\n",
    "\n",
    "$ \\text PDO = \\text Factor * ln(2) $ therefore Factor = $ \\frac {\\text PDO}{\\text ln (2)}$\n",
    "\n",
    "$ \\text Offset = \\text Ceiling Score-(\\text factor * \\text Ln(PDO) $\n",
    "Factor acts as a scaling point to convert the log of odds of an event into score points. Offset acts as a starting credit score. We can calculate Factor and Offset by the above formula\n",
    "\n",
    "Offset = Score - Factor * ln(Odds)\n",
    "Where:\n",
    "\n",
    "    -ùëäùëÇùê∏ = weight of evidence for each grouped attribute\n",
    "    -ùõΩ = regression coefficient for each characteristic\n",
    "    -ùëé = intercept term from logistic regression\n",
    "    -ùëõ = number of characteristics\n",
    "    -ùëò = number of groups (of attributes) in each characteristic\n",
    "    -ùëùùëëùëú = ‚Äúpoints to double odds‚Äù which means  as how much points to increase for the odds of default to become half.\n",
    "    -ùëÇùëëùëëùë† = odds of default .In terms of PD we get odds as (1-PD)/PD\n",
    "    -ùëÜùëêùëúùëüùëí = score where the odds of default is ùëÇùëëùëëùë†.\n",
    "\n",
    "** Min-Max scaling can also be used apart from factor and offset scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82bead5f-6bd0-4c0b-87bf-fb1585cea003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def scaled_score(data, PD, ceiling_score, PDO):\n",
    "    \"\"\"\n",
    "    Create scaled scores for a given dataset.\n",
    "\n",
    "    This function calculates scaled scores based on the provided probability of default (PD) variable\n",
    "    using the provided ceiling score and PDO.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): The dataset containing the PD variable.\n",
    "    PD (str): The name of the probability of default (PD) variable in the dataset.\n",
    "    ceiling_score (float): The ceiling score for transformation.\n",
    "    PDO (float): The increase level for transformation.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The dataset with the scaled score column added.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    odds = (1 - data[PD]) / data[PD]\n",
    "    factor = PDO / np.log(2)\n",
    "    offset_val = ceiling_score - (factor * np.log(PDO))\n",
    "    data['scaled_score'] = offset_val + factor * np.log(odds)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e19d1cf9-cef3-4c56-a98d-55511345235d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>saving_ratio</th>\n",
       "      <th>default_flag</th>\n",
       "      <th>max_dpd_6_months</th>\n",
       "      <th>problematic_cheque_count</th>\n",
       "      <th>duration_in_month</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>installment_as_income_perc</th>\n",
       "      <th>...</th>\n",
       "      <th>business_type_mean_encoded</th>\n",
       "      <th>other_debtors_mean_encoded</th>\n",
       "      <th>purpose_mean_encoded</th>\n",
       "      <th>sector_risk_capped_0.99_woe_encoded</th>\n",
       "      <th>PD</th>\n",
       "      <th>PD_p</th>\n",
       "      <th>PD_Bin</th>\n",
       "      <th>temp</th>\n",
       "      <th>Scores</th>\n",
       "      <th>scaled_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>455</td>\n",
       "      <td>456</td>\n",
       "      <td>2020-12-20</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>2679</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.29989</td>\n",
       "      <td>0.165049</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>0</td>\n",
       "      <td>8.263675e-05</td>\n",
       "      <td>(-0.001, 0.1]</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.400973</td>\n",
       "      <td>3144.837708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>516</td>\n",
       "      <td>517</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1361</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.29989</td>\n",
       "      <td>0.380342</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>0</td>\n",
       "      <td>3.134459e-07</td>\n",
       "      <td>(-0.001, 0.1]</td>\n",
       "      <td>1</td>\n",
       "      <td>-14.975639</td>\n",
       "      <td>3265.475839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>147</td>\n",
       "      <td>148</td>\n",
       "      <td>2021-04-16</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>682</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.29989</td>\n",
       "      <td>0.380342</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>1</td>\n",
       "      <td>6.117294e-01</td>\n",
       "      <td>(0.6, 0.7]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.454587</td>\n",
       "      <td>2931.559176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>789</td>\n",
       "      <td>790</td>\n",
       "      <td>2022-05-13</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>5998</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.29989</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>1</td>\n",
       "      <td>9.818808e-01</td>\n",
       "      <td>(0.9, 1.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>3.992497</td>\n",
       "      <td>2854.997308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>121</td>\n",
       "      <td>122</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>3868</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391061</td>\n",
       "      <td>0.29989</td>\n",
       "      <td>0.165049</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>0</td>\n",
       "      <td>4.651219e-05</td>\n",
       "      <td>(-0.001, 0.1]</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.975750</td>\n",
       "      <td>3157.276110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>637</td>\n",
       "      <td>638</td>\n",
       "      <td>2022-06-10</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>15653</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.29989</td>\n",
       "      <td>0.221429</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>0</td>\n",
       "      <td>2.117288e-01</td>\n",
       "      <td>(0.2, 0.3]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.314536</td>\n",
       "      <td>2969.843762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>425</td>\n",
       "      <td>426</td>\n",
       "      <td>2020-05-16</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>2779</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391061</td>\n",
       "      <td>0.29989</td>\n",
       "      <td>0.165049</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>0</td>\n",
       "      <td>1.192047e-01</td>\n",
       "      <td>(0.1, 0.2]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.999983</td>\n",
       "      <td>2984.677117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>315</td>\n",
       "      <td>316</td>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>2746</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.29989</td>\n",
       "      <td>0.320442</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>1</td>\n",
       "      <td>9.961742e-01</td>\n",
       "      <td>(0.9, 1.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>5.562167</td>\n",
       "      <td>2821.028975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>225</td>\n",
       "      <td>226</td>\n",
       "      <td>2021-04-28</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>2613</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.29989</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>0</td>\n",
       "      <td>2.013052e-04</td>\n",
       "      <td>(-0.001, 0.1]</td>\n",
       "      <td>1</td>\n",
       "      <td>-8.510487</td>\n",
       "      <td>3125.567201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>342</td>\n",
       "      <td>343</td>\n",
       "      <td>2021-02-16</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>3213</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391061</td>\n",
       "      <td>0.29989</td>\n",
       "      <td>0.221429</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>0</td>\n",
       "      <td>1.891949e-07</td>\n",
       "      <td>(-0.001, 0.1]</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.480488</td>\n",
       "      <td>3276.400984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>594 rows √ó 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0   id       date  saving_ratio  default_flag  max_dpd_6_months  \\\n",
       "455         455  456 2020-12-20          0.46             0                 0   \n",
       "516         516  517 2022-09-27          0.04             0                 0   \n",
       "147         147  148 2021-04-16          0.50             0                 0   \n",
       "789         789  790 2022-05-13          0.67             1                 0   \n",
       "121         121  122 2020-03-19          0.22             0                 0   \n",
       "..          ...  ...        ...           ...           ...               ...   \n",
       "637         637  638 2022-06-10          0.21             0                 0   \n",
       "425         425  426 2020-05-16          0.23             0                 0   \n",
       "315         315  316 2023-04-25          0.86             1                 0   \n",
       "225         225  226 2021-04-28          0.31             0                 0   \n",
       "342         342  343 2021-02-16          0.19             0                 0   \n",
       "\n",
       "     problematic_cheque_count  duration_in_month  credit_amount  \\\n",
       "455                         3                 24           2679   \n",
       "516                         0                  6           1361   \n",
       "147                         0                 12            682   \n",
       "789                         2                 40           5998   \n",
       "121                         1                 24           3868   \n",
       "..                        ...                ...            ...   \n",
       "637                         1                 60          15653   \n",
       "425                         0                 18           2779   \n",
       "315                         1                 36           2746   \n",
       "225                         0                 36           2613   \n",
       "342                         1                 18           3213   \n",
       "\n",
       "     installment_as_income_perc  ...  business_type_mean_encoded  \\\n",
       "455                           4  ...                    0.260870   \n",
       "516                           2  ...                    0.260870   \n",
       "147                           4  ...                    0.260870   \n",
       "789                           4  ...                    0.260870   \n",
       "121                           4  ...                    0.391061   \n",
       "..                          ...  ...                         ...   \n",
       "637                           2  ...                    0.260870   \n",
       "425                           1  ...                    0.391061   \n",
       "315                           4  ...                    0.260870   \n",
       "225                           4  ...                    0.260870   \n",
       "342                           1  ...                    0.391061   \n",
       "\n",
       "     other_debtors_mean_encoded  purpose_mean_encoded  \\\n",
       "455                     0.29989              0.165049   \n",
       "516                     0.29989              0.380342   \n",
       "147                     0.29989              0.380342   \n",
       "789                     0.29989              0.440000   \n",
       "121                     0.29989              0.165049   \n",
       "..                          ...                   ...   \n",
       "637                     0.29989              0.221429   \n",
       "425                     0.29989              0.165049   \n",
       "315                     0.29989              0.320442   \n",
       "225                     0.29989              0.363636   \n",
       "342                     0.29989              0.221429   \n",
       "\n",
       "     sector_risk_capped_0.99_woe_encoded  PD          PD_p         PD_Bin  \\\n",
       "455                            -0.001891   0  8.263675e-05  (-0.001, 0.1]   \n",
       "516                            -0.001891   0  3.134459e-07  (-0.001, 0.1]   \n",
       "147                            -0.001891   1  6.117294e-01     (0.6, 0.7]   \n",
       "789                            -0.001891   1  9.818808e-01     (0.9, 1.0]   \n",
       "121                            -0.001891   0  4.651219e-05  (-0.001, 0.1]   \n",
       "..                                   ...  ..           ...            ...   \n",
       "637                            -0.001891   0  2.117288e-01     (0.2, 0.3]   \n",
       "425                            -0.001891   0  1.192047e-01     (0.1, 0.2]   \n",
       "315                            -0.001891   1  9.961742e-01     (0.9, 1.0]   \n",
       "225                            -0.001891   0  2.013052e-04  (-0.001, 0.1]   \n",
       "342                            -0.001891   0  1.891949e-07  (-0.001, 0.1]   \n",
       "\n",
       "     temp     Scores  scaled_score  \n",
       "455     1  -9.400973   3144.837708  \n",
       "516     1 -14.975639   3265.475839  \n",
       "147     1   0.454587   2931.559176  \n",
       "789     1   3.992497   2854.997308  \n",
       "121     1  -9.975750   3157.276110  \n",
       "..    ...        ...           ...  \n",
       "637     1  -1.314536   2969.843762  \n",
       "425     1  -1.999983   2984.677117  \n",
       "315     1   5.562167   2821.028975  \n",
       "225     1  -8.510487   3125.567201  \n",
       "342     1 -15.480488   3276.400984  \n",
       "\n",
       "[594 rows x 71 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_score(train_data_logistic,PD,3000,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2d197f3-7a29-4105-8430-73f14d3bd65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can observe above that the scaled score surpasses the ceiling score and hence needs calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54942657-0a30-4b0c-8ac7-ad47c0be066a",
   "metadata": {},
   "source": [
    "# Bayesian Calibration\n",
    "**Priors:** Existing knowledge of model coefficients against probability of default values often known as Probability Distributions.\n",
    "\n",
    "**Data Collection:** New Data Point acts as the basis of predicting the likelyhood of default for given model parameters.(central_tendency)\n",
    "\n",
    "**Bayes Inference:** Using Bayes' Theorem, the prior probabilities are combined with the likelihood from the data to produce posterior probabilities that aling each of the individual values in PD to ajust such that the average value of PD matches that of the model parameter (central_tendency)\n",
    "Instead of single point estimates, Bayesian methods yield probability distributions for model parameters, reflecting uncertainty.\n",
    "\n",
    "**Calibration:** The process adjusts the model's output (like default probabilities) to align with observed outcomes, often using techniques like Expected Maximization (EM) or optimizing scoring rules to match true posterior distributions.\n",
    "\n",
    "**Self-Learning:** As new loan data comes in, the model updates its posterior distributions, allowing it to adapt and improve its predictions over time without needing complete retraining\n",
    "\n",
    "# Key Advantages\n",
    "\n",
    "    -Incorporates Prior Info: Banks can use historical insights or regulatory guidance, improving models even with limited new data.\n",
    "    -Handles Uncertainty: Provides a full probability distribution for risk, not just a single number, better reflecting true risk.\n",
    "    -Data-Driven & Objective: Leverages bank's own data for bespoke models, reducing reliance on subjective human judgment.\n",
    "    -Improved Performance: Often outperforms traditional single-stage methods (like standard logistic regression) in prediction accuracy (e.g., higher KS, AUC).\n",
    "    -Self-Learning: Adapts to evolving economic conditions and customer behaviors. \n",
    "\n",
    "**Mechanism:** It typically scales a \"Point-in-Time\" (PIT) probability to a \"Through-the-Cycle\" (TTC) level using a scaling factor derived from the portfolio's central tendency.\n",
    "Example (Conceptual):\n",
    "Suppose a rating grade has a prior expected PD of 1% (from 10 years of data). In the current year, only 2 defaults are observed out of 500 loans (0.4%). Bayesian methods blend these to find a posterior PD (e.g., 0.8%), preventing drastic model shifts due to one \"lucky\" year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e00c0-84da-40f7-8017-8db00160b3bd",
   "metadata": {},
   "source": [
    "The Calibration involves adjusting the PD looking at the pre calculated target i.e central_tendecy of the portfolio.\n",
    "The step by step process flow is coded as below:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60dc6e5-79a1-4122-9046-a14d5cf36c39",
   "metadata": {},
   "source": [
    "Implementing Bayesian calibration for scaled credit scores involves two main phases: Mapping scores back to Point-in-Time (PIT) probabilities and Adjusting those probabilities to a Through-the-Cycle (TTC) target using a Bayesian scaling factor.\n",
    "\n",
    "To simplify this process, we break the Bayesian calibration into four logical blocks. This method adjusts  existing scores (already scaled via factor and offset) to ensure the final Probability of Default (PD) aligns with a target Central Tendency (the long-run average default rate expected by the bank or regulator)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68617e58-871d-4f68-b766-093bf741e8c6",
   "metadata": {},
   "source": [
    "Block 1: Calculating Portfolio-Wide Observed PD\n",
    "Before calibrating, we must find the current average PD of your master scale that has already been calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f32187d6-fdbb-4efd-931e-79d176acaf8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final.PD.Range</th>\n",
       "      <th>Total.Observations</th>\n",
       "      <th>Total.Distr</th>\n",
       "      <th>Good.Count</th>\n",
       "      <th>Bad.Count</th>\n",
       "      <th>Good.Distr</th>\n",
       "      <th>Bad.Distr</th>\n",
       "      <th>Bad.Rate</th>\n",
       "      <th>PD</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-0.001, 0.1]</td>\n",
       "      <td>370</td>\n",
       "      <td>0.622896</td>\n",
       "      <td>370</td>\n",
       "      <td>0</td>\n",
       "      <td>0.622896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006688</td>\n",
       "      <td>500.071105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.1, 0.2]</td>\n",
       "      <td>17</td>\n",
       "      <td>0.028620</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0.023569</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.137085</td>\n",
       "      <td>183.971446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.2, 0.3]</td>\n",
       "      <td>12</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013468</td>\n",
       "      <td>0.006734</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.225923</td>\n",
       "      <td>123.147919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.3, 0.4]</td>\n",
       "      <td>7</td>\n",
       "      <td>0.011785</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006734</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.362715</td>\n",
       "      <td>56.359834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.4, 0.5]</td>\n",
       "      <td>6</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.469115</td>\n",
       "      <td>12.369724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(0.5, 0.6]</td>\n",
       "      <td>7</td>\n",
       "      <td>0.011785</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.535657</td>\n",
       "      <td>-14.287230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0.6, 0.7]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.016835</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008418</td>\n",
       "      <td>0.008418</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.654010</td>\n",
       "      <td>-63.671163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(0.7, 0.8]</td>\n",
       "      <td>12</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.008418</td>\n",
       "      <td>0.011785</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.753022</td>\n",
       "      <td>-111.479456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(0.8, 0.9]</td>\n",
       "      <td>19</td>\n",
       "      <td>0.031987</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.858946</td>\n",
       "      <td>-180.656335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(0.9, 1.0]</td>\n",
       "      <td>134</td>\n",
       "      <td>0.225589</td>\n",
       "      <td>3</td>\n",
       "      <td>131</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.220539</td>\n",
       "      <td>0.977612</td>\n",
       "      <td>0.981224</td>\n",
       "      <td>-395.621415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Final.PD.Range  Total.Observations  Total.Distr  Good.Count  Bad.Count  \\\n",
       "0  (-0.001, 0.1]                 370     0.622896         370          0   \n",
       "1     (0.1, 0.2]                  17     0.028620          14          3   \n",
       "2     (0.2, 0.3]                  12     0.020202           8          4   \n",
       "3     (0.3, 0.4]                   7     0.011785           4          3   \n",
       "4     (0.4, 0.5]                   6     0.010101           3          3   \n",
       "5     (0.5, 0.6]                   7     0.011785           1          6   \n",
       "6     (0.6, 0.7]                  10     0.016835           5          5   \n",
       "7     (0.7, 0.8]                  12     0.020202           5          7   \n",
       "8     (0.8, 0.9]                  19     0.031987           1         18   \n",
       "9     (0.9, 1.0]                 134     0.225589           3        131   \n",
       "\n",
       "   Good.Distr  Bad.Distr  Bad.Rate        PD       Score  \n",
       "0    0.622896   0.000000  0.000000  0.006688  500.071105  \n",
       "1    0.023569   0.005051  0.176471  0.137085  183.971446  \n",
       "2    0.013468   0.006734  0.333333  0.225923  123.147919  \n",
       "3    0.006734   0.005051  0.428571  0.362715   56.359834  \n",
       "4    0.005051   0.005051  0.500000  0.469115   12.369724  \n",
       "5    0.001684   0.010101  0.857143  0.535657  -14.287230  \n",
       "6    0.008418   0.008418  0.500000  0.654010  -63.671163  \n",
       "7    0.008418   0.011785  0.583333  0.753022 -111.479456  \n",
       "8    0.001684   0.030303  0.947368  0.858946 -180.656335  \n",
       "9    0.005051   0.220539  0.977612  0.981224 -395.621415  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_scale_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53661d87-51cb-4a65-87ab-6e386069f79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "# 1. Calculate the weighted average PD of the current master scale\n",
    "total_obs = master_scale_data[\"Total.Observations\"].sum()\n",
    "total_defaults = (master_scale_data[\"Total.Observations\"] * master_scale_data[\"PD\"]).sum()\n",
    "avg_pd = total_defaults / total_obs\n",
    "central_tendency=0.5\n",
    "# This is the \"Point-in-Time\" (PIT) average. We multiply the PD of each grade(bin) by the number of observations in that grade to find the total expected defaults, \n",
    "# then divide by the total population to get the average pd for that bin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a075be-17fe-40f6-a62b-fb44cc9ee020",
   "metadata": {},
   "source": [
    "Block 2: The Bayesian Shift (The \"Scales\" Adjustment)\n",
    "This is the core Bayesian step. We adjust the individual PDs of each rating grade(bin) so that the entire portfolio's average shifts to match the central_tendency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ada60cf-b75e-4bb4-b168-7c05dacebbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Adjust the individual PDs for each observation using the Bayesian formula (Scales Shift)\n",
    "# Prior: central_tendency / avg_pd\n",
    "# Likelihood: The existing PD\n",
    "numerator = (master_scale_data[\"PD\"] * central_tendency / avg_pd)\n",
    "denominator = numerator + (1 - master_scale_data[\"PD\"]) * ((1 - central_tendency) / (1 - avg_pd))\n",
    "\n",
    "master_scale_data['calibrated_pd'] = numerator / denominator\n",
    "# The central_tendency variable in your code acts as the \"anchor\". If your current avg_pd (calculated from your model) is 2% but your historical central\n",
    "#_tendency is 3%, the Bayesian calibration will shift all individual PDs upward to ensure the final portfolio average matches that 3% target\n",
    "# 3. Convert calibrated PD to Log-Odds and find the linear relationship\n",
    "# Calculate the target Odds Ratio\n",
    "master_scale_data['OddRatio'] = (1 - master_scale_data['calibrated_pd']) / master_scale_data['calibrated_pd']\n",
    "\n",
    "# Reset index to ensure a clean 0 to N range for both\n",
    "master_scale_data = master_scale_data.reset_index(drop=True)\n",
    "\n",
    "# Define X and y from the same cleaned DataFrame\n",
    "X = sm.add_constant(master_scale_data[\"Score\"])\n",
    "y = np.log(master_scale_data['OddRatio']) # Ensure this column is in master_scale_data\n",
    "\n",
    "OLS_model = sm.OLS(y, X).fit()\n",
    "intercept = OLS_model.params.iloc[0]\n",
    "score_coef = OLS_model.params.iloc[1]\n",
    "#  The relationship between a score and PD is non-linear (logistic). \n",
    "# By taking the log of the odds, we create a linear target. \n",
    "# The Regression finds the Intercept and Coefficient that best maps your scaled scores to these new Bayesian-adjusted probabilities.\n",
    "# 4. Map the formula back to the individual scores\n",
    "# Formula: PD = 1 / (1 + exp(Intercept + Score * Coef))\n",
    "linear_component = intercept +(train_data_logistic[[\"scaled_score\"]] * score_coef)\n",
    "train_data_logistic['calibrated_pd'] = 1 / (1 + np.exp(linear_component))\n",
    "# This is the Logistic Sigmoid Function. It converts the linear output of the regression back into a probability between 0 and 1. \n",
    "# This ensures that every customer score now results in a PD that is consistent with the bank's required Central Tendency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e94a0786-ac0d-4ab0-acdf-d6469a319894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Map the formula back to the individual scores\n",
    "# Formula: PD = 1 / (1 + exp(Intercept + Score * Coef))\n",
    "linear_component = intercept +(train_data_logistic[[\"scaled_score\"]] * score_coef)\n",
    "train_data_logistic['calibrated_pd'] = 1 / (1 + np.exp(linear_component))\n",
    "# This is the Logistic Sigmoid Function. It converts the linear output of the regression back into a probability between 0 and 1. \n",
    "# This ensures that every customer score now results in a PD that is consistent with the bank's required Central Tendency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2979a852-c0c8-4829-8ef1-fd6e325c9cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     scaled_score  calibrated_pd\n",
      "455   3144.837708   5.056757e-14\n",
      "516   3265.475839   1.513378e-14\n",
      "147   2931.559176   4.267062e-13\n",
      "789   2854.997308   9.175567e-13\n",
      "121   3157.276110   4.465322e-14\n"
     ]
    }
   ],
   "source": [
    "# Ensure parameters from previous steps are ready\n",
    "intercept = OLS_model.params.iloc[0]\n",
    "score_coef = OLS_model.params.iloc[1]\n",
    "calibration_data=train_data_logistic\n",
    "# 4. Map the formula back to individual customer records\n",
    "# Linear Component (z) = Intercept + (Score * Coefficient)\n",
    "linear_component = intercept + (train_data_logistic[\"scaled_score\"] * score_coef)\n",
    "\n",
    "# Logistic Sigmoid Function: 1 / (1 + exp(z))\n",
    "# This converts the linear log-odds into a valid PD (0 to 1)\n",
    "calibration_data['calibrated_pd'] = 1 / (1 + np.exp(linear_component))\n",
    "\n",
    "# Verify the results\n",
    "print(calibration_data[['scaled_score', 'calibrated_pd']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8190cd63-2769-459a-a03b-f233462e07c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default_flag</th>\n",
       "      <th>Net_Profit_Margin</th>\n",
       "      <th>ROA</th>\n",
       "      <th>sector_risk</th>\n",
       "      <th>ROE</th>\n",
       "      <th>saving_ratio</th>\n",
       "      <th>modelpd</th>\n",
       "      <th>modelscore</th>\n",
       "      <th>calibrated_pd</th>\n",
       "      <th>calibrated_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>-8</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.966126e-08</td>\n",
       "      <td>-17.744616</td>\n",
       "      <td>1.282577e-10</td>\n",
       "      <td>-22.776980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.041562e-06</td>\n",
       "      <td>-13.774788</td>\n",
       "      <td>1.952784e-08</td>\n",
       "      <td>-17.751425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.674918e-04</td>\n",
       "      <td>-8.694409</td>\n",
       "      <td>1.212804e-05</td>\n",
       "      <td>-11.319978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>0.46</td>\n",
       "      <td>4.827540e-04</td>\n",
       "      <td>-7.635520</td>\n",
       "      <td>4.633847e-05</td>\n",
       "      <td>-9.979492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.49</td>\n",
       "      <td>6.585305e-08</td>\n",
       "      <td>-16.535840</td>\n",
       "      <td>5.924558e-10</td>\n",
       "      <td>-21.246745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    default_flag  Net_Profit_Margin  ROA  sector_risk  ROE  saving_ratio  \\\n",
       "22             0               -9.0   35            3   -8          0.12   \n",
       "26             0               23.0   26            0   67          0.12   \n",
       "32             0               62.0   20            2   37          0.07   \n",
       "46             0               43.0   48            1   94          0.46   \n",
       "58             0               -5.0   35            1   25          0.49   \n",
       "\n",
       "         modelpd  modelscore  calibrated_pd  calibrated_score  \n",
       "22  1.966126e-08  -17.744616   1.282577e-10        -22.776980  \n",
       "26  1.041562e-06  -13.774788   1.952784e-08        -17.751425  \n",
       "32  1.674918e-04   -8.694409   1.212804e-05        -11.319978  \n",
       "46  4.827540e-04   -7.635520   4.633847e-05         -9.979492  \n",
       "58  6.585305e-08  -16.535840   5.924558e-10        -21.246745  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibration_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a3b5b3d-80b4-4d9b-862f-454075423293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'calibration_data' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store calibration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a4fc276-7419-423d-ba0d-0eaa0afbd497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               OddRatio   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                  1.000\n",
      "Method:                 Least Squares   F-statistic:                 3.124e+31\n",
      "Date:                Wed, 21 Jan 2026   Prob (F-statistic):          1.18e-123\n",
      "Time:                        17:12:23   Log-Likelihood:                 329.89\n",
      "No. Observations:                  10   AIC:                            -655.8\n",
      "Df Residuals:                       8   BIC:                            -655.2\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.8329   4.03e-16  -2.07e+15      0.000      -0.833      -0.833\n",
      "Score          0.0100   1.79e-18   5.59e+15      0.000       0.010       0.010\n",
      "==============================================================================\n",
      "Omnibus:                       15.175   Durbin-Watson:                   0.511\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):                7.839\n",
      "Skew:                           1.888   Prob(JB):                       0.0199\n",
      "Kurtosis:                       5.133   Cond. No.                         226.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(OLS_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63595cda-0d76-494a-86ed-ab0381c5756a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['const', 'Score']\n"
     ]
    }
   ],
   "source": [
    "feature_names = OLS_model.model.exog_names\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b66b5f50-41fb-421c-bbb0-ad3d7c35411a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OddRatio\n"
     ]
    }
   ],
   "source": [
    "# Returns the name of the target variable (y)\n",
    "target_name = OLS_model.model.endog_names\n",
    "print(target_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e360a1-6b93-4cfc-a5ac-070fa610dcd6",
   "metadata": {},
   "source": [
    "In credit risk calibration, OLS regression is a tool to find the link between Scores and Log-Odds. Interpreting the results involves looking at the parameters that define the calibration curve and the metrics that prove its reliability.¬†\n",
    "1. The Intercept and Coefficient (The Calibration Parameters)¬†These values are the \"DNA\" of your calibration formula: $\\text (LogOdds=Intercept+(Score * Coefficient)$).\n",
    "2. Intercept: This represents the baseline log-odds when the score is zero. In practical credit risk terms, it is a mathematical anchor that positions the PD curve vertically to ensure the portfolio's average PD matches your Central Tendency.Score\n",
    "3. Coefficient: This represents the sensitivity of risk to the score.Direction: A negative coefficient is standard for credit scores (where higher scores = lower risk). It means for every 1-unit increase in score, the log-odds of default decrease.Magnitude: This tells you the rate of change. If the coefficient is -0.05, a 100-point increase in score results in a 5-unit drop in log-odds.\n",
    "4. P-Values and Significance¬†P > |t|: This is the most critical diagnostic. You want a p-value less than 0.05 for your score coefficient.Meaning: A p-value < 0.05 indicates that your Score is a statistically significant predictor of the target PD. If it is higher, your model likely lacks \"calibration power\"‚Äîmeaning the score doesn't reliably distinguish between different levels of risk.\n",
    "5. R-Squared (Goodness of Fit)¬†: Measures the proportion of variance in the log-odds explained by your scores.Credit Risk Context: In calibration (unlike initial model building), we often see very high $ (\\text R^{2})$ (0.90+) because the master scale is already smoothed and rank-ordered. A low $ (\\text R^{2})$ here suggests that your \"scaled scores\" are not linearly related to the log-odds, potentially requiring a different calibration method (like Isotonic Regression).\n",
    "6. Diagnostic Metrics for Models¬†F-Statistic: Tests the overall significance of the model. A high F-statistic (and low Prob F-stat) confirms that the entire regression is better than a random guess.\n",
    "7. Durbin-Watson: Should be close to 2.0. This confirms there is no \"autocorrelation\" in your rating grades‚Äîmeaning the risk jump between Grade 1 and Grade 2 is independent of the jump between Grade 2 and Grade 3.\n",
    "8. Confidence Intervals ([0.025, 0.975]): For a stable model, these intervals should be narrow. If they are very wide, your calibration may be unstable and could shift drastically with small changes in the data.¬†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14569b27-5252-4ac4-ac14-aeaf63d51685",
   "metadata": {},
   "source": [
    "Since we have already obtained the scores and hence the probability distribution in the form of calibrated_PD aligned with model parameters which are predetermined either through historic data or through model governance,we can derive the default flag variable for each of these scores.\n",
    "To derive the default flag we would calculate the Probabilities for each of the scores and these probabilities can be further converted into 1's and 0's.\n",
    "Regressing the calibrated score with the default flag to get the calibrated_PD= $\\frac {1}{1+e^(-CalibratedScore)}$ \n",
    "Implementation over the calibrated_PD is done as coded below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab5bef09-4e26-4d3d-872c-e7937269e198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no stored variable or alias #\n",
      "no stored variable or alias The\n",
      "no stored variable or alias Logistic\n",
      "no stored variable or alias Regression\n",
      "no stored variable or alias trained\n",
      "no stored variable or alias on\n",
      "no stored variable or alias the\n",
      "no stored variable or alias data\n",
      "no stored variable or alias stored\n",
      "no stored variable or alias in\n",
      "no stored variable or alias part\n",
      "no stored variable or alias 2\n",
      "no stored variable or alias is\n",
      "no stored variable or alias imported\n"
     ]
    }
   ],
   "source": [
    "%store -r model # The Logistic Regression model trained on the data stored in part 2 is imported "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e808aec0-db05-4e4b-b5da-e7febe9af02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Net_Profit_Margin', 'ROA', 'sector_risk', 'ROE', 'saving_ratio'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_names_in_# These are the set of features over which the LR model was trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38e51149-d5b3-466f-821d-6495ecdfd660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def regression_calibration(model, calibration_data, default_flag):\n",
    "    # Calculate model predicted probabilities\n",
    "    calibration_data['modelpd'] = model.predict_proba(calibration_data.drop(default_flag, axis=1))[:, 1] # The trained LR model on traing data is used to predict the PD and stored as model_pd on the calibration data\n",
    "\n",
    "    # Calculate model scores\n",
    "    calibration_data['modelscore'] = np.log(calibration_data['modelpd'] / (1 - calibration_data['modelpd'])) # Convert the [predicted PD values in calibration data into log odds or modelscores\n",
    "\n",
    "    # Fit calibration model\n",
    "    calibration_model = LogisticRegression()\n",
    "    calibration_model.fit(calibration_data[['modelscore']], calibration_data[default_flag]) # fit the calibration model according to the modelscores as calculated from the above step\n",
    "\n",
    "    # Calculate calibrated probabilities\n",
    "    calibration_data['calibrated_pd'] = calibration_model.predict_proba(calibration_data[['modelscore']])[:, 1] # Predict the PD for the calculated modelscores\n",
    "\n",
    "    # Calculate calibrated scores\n",
    "    calibration_data['calibrated_score'] = np.log(calibration_data['calibrated_pd'] / (1 - calibration_data['calibrated_pd']))# convert the predicted PDs as log odds as the calinrated score\n",
    "\n",
    "    # Prepare formula strings\n",
    "    var_names = calibration_data.columns[:-4]  # Exclude modelpd, modelscore, calibrated_pd, calibrated_score\n",
    "    equation_str = \" + \".join([f\"{coef:.7f}*{var_name}\" for coef, var_name in zip(model.coef_[0], var_names)])\n",
    "    equation_str = f\"{equation_str} + {model.intercept_[0]}\"\n",
    "    equation_str = f\"{model.coef_[0]}*{var_names} + {model.intercept_[0]}\"\n",
    "\n",
    "    var_names_cal = calibration_data.columns[:-6]  # Exclude modelpd, modelscore, calibrated_pd, calibrated_score, default_flag\n",
    "    equation_str_cal = \" + \".join([f\"{coef:.7f}*{var_name}\" for coef, var_name in zip(calibration_model.coef_[0], var_names_cal)])\n",
    "    equation_str_cal = f\"{equation_str_cal} + {calibration_model.intercept_[0]}\"\n",
    "\n",
    "    # Prepare calibration formula\n",
    "    calibration_formula = f\"modelscore formula ::: {equation_str}, calibrated_score formula ::: {equation_str_cal}, Calibration Formula to get calibrated_pd ::: 1/(1 + exp(-calibrated_score))\"\n",
    "\n",
    "    return calibration_data,calibration_model,calibration_formula\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d68a6a2-a57c-4fca-b71e-353689407e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelscore formula ::: [0.13210864 0.0554701  0.61455298 0.02780321 2.68092172]*Index(['default_flag', 'Net_Profit_Margin', 'ROA', 'sector_risk', 'ROE',\n",
      "       'saving_ratio'],\n",
      "      dtype='object') + -20.44003516377127, calibrated_score formula ::: 1.2659380*default_flag + -0.3133958970876711, Calibration Formula to get calibrated_pd ::: 1/(1 + exp(-calibrated_score))\n",
      "LogisticRegression()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_17048\\2735133407.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  calibration_data['modelpd'] = model.predict_proba(calibration_data.drop(default_flag, axis=1))[:, 1] # The trained LR model on traing data is used to predict the PD and stored as model_pd on the calibration data\n",
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_17048\\2735133407.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  calibration_data['modelscore'] = np.log(calibration_data['modelpd'] / (1 - calibration_data['modelpd'])) # Convert the [predicted PD values in calibration data into log odds or modelscores\n",
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_17048\\2735133407.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  calibration_data['calibrated_pd'] = calibration_model.predict_proba(calibration_data[['modelscore']])[:, 1] # Predict the PD for the calculated modelscores\n",
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_17048\\2735133407.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  calibration_data['calibrated_score'] = np.log(calibration_data['calibrated_pd'] / (1 - calibration_data['calibrated_pd']))# convert the predicted PDs as log odds as the calinrated score\n"
     ]
    }
   ],
   "source": [
    "calibration_data, calibration_model, calibration_formula = regression_calibration(\n",
    "    model, \n",
    "    validation_data_logistic[['default_flag','Net_Profit_Margin', 'ROA', 'sector_risk', 'ROE', 'saving_ratio']], \n",
    "    \"default_flag\"\n",
    ")\n",
    "print(calibration_formula)\n",
    "print(calibration_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51758ee5-7a20-429a-8466-b288456ffdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default_flag</th>\n",
       "      <th>Net_Profit_Margin</th>\n",
       "      <th>ROA</th>\n",
       "      <th>sector_risk</th>\n",
       "      <th>ROE</th>\n",
       "      <th>saving_ratio</th>\n",
       "      <th>modelpd</th>\n",
       "      <th>modelscore</th>\n",
       "      <th>calibrated_pd</th>\n",
       "      <th>calibrated_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>-8</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.966126e-08</td>\n",
       "      <td>-17.744616</td>\n",
       "      <td>1.282577e-10</td>\n",
       "      <td>-22.776980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.041562e-06</td>\n",
       "      <td>-13.774788</td>\n",
       "      <td>1.952784e-08</td>\n",
       "      <td>-17.751425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.674918e-04</td>\n",
       "      <td>-8.694409</td>\n",
       "      <td>1.212804e-05</td>\n",
       "      <td>-11.319978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>0.46</td>\n",
       "      <td>4.827540e-04</td>\n",
       "      <td>-7.635520</td>\n",
       "      <td>4.633847e-05</td>\n",
       "      <td>-9.979492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.49</td>\n",
       "      <td>6.585305e-08</td>\n",
       "      <td>-16.535840</td>\n",
       "      <td>5.924558e-10</td>\n",
       "      <td>-21.246745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    default_flag  Net_Profit_Margin  ROA  sector_risk  ROE  saving_ratio  \\\n",
       "22             0               -9.0   35            3   -8          0.12   \n",
       "26             0               23.0   26            0   67          0.12   \n",
       "32             0               62.0   20            2   37          0.07   \n",
       "46             0               43.0   48            1   94          0.46   \n",
       "58             0               -5.0   35            1   25          0.49   \n",
       "\n",
       "         modelpd  modelscore  calibrated_pd  calibrated_score  \n",
       "22  1.966126e-08  -17.744616   1.282577e-10        -22.776980  \n",
       "26  1.041562e-06  -13.774788   1.952784e-08        -17.751425  \n",
       "32  1.674918e-04   -8.694409   1.212804e-05        -11.319978  \n",
       "46  4.827540e-04   -7.635520   4.633847e-05         -9.979492  \n",
       "58  6.585305e-08  -16.535840   5.924558e-10        -21.246745  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibration_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c254d6d5-7219-458f-bb9b-728de7b9154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'calibration_data' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store calibration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221481f8-f48a-43bf-9d7a-de9cc716a8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
